{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lorfs.csv\n",
    "lorfs_short = pd.read_csv('Data/Test/lorfs_short.csv')\n",
    "lorfs_medium = pd.read_csv('Data/Test/lorfs_medium.csv')\n",
    "lorfs_long = pd.read_csv('Data/Test/lorfs_long.csv')\n",
    "\n",
    "# sort by length\n",
    "lorfs_short = lorfs_short.sort_values(by='length')\n",
    "lorfs_medium = lorfs_medium.sort_values(by='length')\n",
    "lorfs_long = lorfs_long.sort_values(by='length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_non_aa(df):\n",
    "    non_amino_acids = ['\\*','x','0','1',';','5','4','7','8','9','>','_','\\.','2','\\-','3','6']\n",
    "    pattern = '|'.join(non_amino_acids)\n",
    "\n",
    "    print(\"Original shape:\", df.shape)\n",
    "    orginal_shape = df.shape\n",
    "    # Make sure the sequence column doesn't contain any of these characters\n",
    "    df = df[~df['Sequence'].str.contains(pattern, regex=True, case=False, na=False)]\n",
    "    # Print new shape\n",
    "    print(\"New shape after filtering:\", df.shape)\n",
    "    print(\"Number of removed rows:\", orginal_shape[0] - df.shape[0])\n",
    "    print(\"\\n\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def one_hot_encode_sequences(df, padding_length):\n",
    "    df = filter_non_aa(df)\n",
    "    tokenizer = Tokenizer(char_level=True)\n",
    "    tokenizer.fit_on_texts(df['Sequence'])\n",
    "    sequences_numeric = tokenizer.texts_to_sequences(df['Sequence'])\n",
    "    sequences_padded = pad_sequences(sequences_numeric, maxlen=padding_length, padding='post', truncating='post')\n",
    "    one_hot_sequences = np.zeros((len(sequences_padded), padding_length, (len(tokenizer.word_index) + 1)))\n",
    "    for i, sequence in enumerate(sequences_padded):\n",
    "        for j, char_index in enumerate(sequence):\n",
    "            if char_index != 0:  # Skip padding\n",
    "                one_hot_sequences[i, j, char_index] = 1\n",
    "    one_hot_sequences = one_hot_sequences[:,:,1:]\n",
    "    return one_hot_sequences, df['has_hmm'], pd.DataFrame(df['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot_encode_sequences\n",
    "lorfs_short_one_hot, lorfs_short_target, lorfs_short_len = one_hot_encode_sequences(lorfs_short, 100)\n",
    "lorfs_medium_one_hot, lorfs_medium_target, lorfs_medium_len = one_hot_encode_sequences(lorfs_medium, 400)\n",
    "lorfs_long_one_hot, lorfs_long_target, lorfs_long_len = one_hot_encode_sequences(lorfs_long, 1000)\n",
    "\n",
    "# flatten the one hot encoded sequences\n",
    "lorfs_short_one_hot_flat = lorfs_short_one_hot.reshape(lorfs_short_one_hot.shape[0], -1)\n",
    "lorfs_medium_one_hot_flat = lorfs_medium_one_hot.reshape(lorfs_medium_one_hot.shape[0], -1)\n",
    "lorfs_long_one_hot_flat = lorfs_long_one_hot.reshape(lorfs_long_one_hot.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def predict_and_evaluate(models, data, targets):\n",
    "    \"\"\"\n",
    "    Predicts outcomes using the given models and evaluates their accuracy.\n",
    "    \n",
    "    Parameters:\n",
    "    - models: A dictionary of loaded models keyed by dataset size ('short', 'medium', 'long').\n",
    "    - data: A dictionary containing the preprocessed input data for each model, keyed by dataset size.\n",
    "    - targets: A dictionary containing the true labels for each dataset size.\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary containing the accuracy scores keyed by dataset size.\n",
    "    \"\"\"\n",
    "    accuracies = {}\n",
    "    \n",
    "    for size in models.keys():\n",
    "        # Make predictions\n",
    "        predictions = models[size].predict(data[size])\n",
    "        \n",
    "        # Convert predictions to binary\n",
    "        binary_predictions = [1 if i > 0.5 else 0 for i in predictions.ravel()]\n",
    "        \n",
    "        # Calculate and store accuracy\n",
    "        accuracy = accuracy_score(targets[size], binary_predictions)\n",
    "        accuracies[size] = accuracy\n",
    "        print(f'{size.capitalize()} dataset accuracy: {accuracy}')\n",
    "    \n",
    "    return accuracies\n",
    "\n",
    "# Example of how to use the function\n",
    "# Assuming lorfs_short_one_hot_flat, lorfs_medium_one_hot_flat, lorfs_long_one_hot_flat, \n",
    "# lorfs_short_target, lorfs_medium_target, lorfs_long_target are defined\n",
    "\n",
    "data = {\n",
    "    'short': lorfs_short_one_hot_flat,\n",
    "    'medium': lorfs_medium_one_hot_flat,\n",
    "    'long': lorfs_long_one_hot_flat\n",
    "}\n",
    "\n",
    "targets = {\n",
    "    'short': lorfs_short_target,\n",
    "    'medium': lorfs_medium_target,\n",
    "    'long': lorfs_long_target\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directory where the models are saved\n",
    "base_dir = 'Models/Dense'\n",
    "\n",
    "# Model names based on the dataset size\n",
    "dataset_sizes = ['short', 'medium', 'long']\n",
    "\n",
    "# Initialize a dictionary to hold the loaded models\n",
    "models = {}\n",
    "\n",
    "# Loop through the dataset sizes, load each model, and add it to the dictionary\n",
    "for size in dataset_sizes:\n",
    "    model_name = f'{size}_dataset_model.keras'\n",
    "    model_path = os.path.join(base_dir, model_name)\n",
    "    models[size] = load_model(model_path)\n",
    "    print(f'{size.capitalize()} dataset model loaded from {model_path}')\n",
    "\n",
    "# Call the function with the models, data, and targets\n",
    "accuracies = predict_and_evaluate(models, data, targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "data = {\n",
    "    'short': lorfs_short_len,\n",
    "    'medium': lorfs_medium_len,\n",
    "    'long': lorfs_long_len\n",
    "}\n",
    "\n",
    "# Define the base directory where the models are saved\n",
    "base_dir = 'Models/Logreg'\n",
    "\n",
    "# Model names based on the dataset size\n",
    "dataset_sizes = ['short', 'medium', 'long']\n",
    "\n",
    "# Initialize a dictionary to hold the loaded models\n",
    "models = {}\n",
    "\n",
    "# Loop through the dataset sizes, load each model, and add it to the dictionary\n",
    "for size in dataset_sizes:\n",
    "    model_name = f'{size}_dataset_model.joblib'\n",
    "    model_path = os.path.join(base_dir, model_name)\n",
    "    models[size] = joblib.load(model_path)  # Corrected model loading function\n",
    "    print(f'{size.capitalize()} dataset model loaded from {model_path}')\n",
    "\n",
    "# Call the function with the models, data, and targets\n",
    "accuracies = predict_and_evaluate(models, data, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
